{
  "tests": [
    {
      "name": "basic_text_to_image",
      "input": {
        "workflow": {
          "1": {
            "class_type": "UNETLoader",
            "inputs": {
              "unet_name": "qwen_image_2512_fp8_e4m3fn_scaled_comfyui_4steps_v1.0.safetensors",
              "weight_dtype": "fp8_e4m3fn"
            },
            "_meta": {
              "title": "Load Diffusion Model"
            }
          },
          "2": {
            "class_type": "CLIPLoader",
            "inputs": {
              "clip_name": "qwen_2.5_vl_7b_fp8_scaled.safetensors",
              "type": "lumina2",
              "device": "default"
            },
            "_meta": {
              "title": "Load CLIP"
            }
          },
          "3": {
            "class_type": "VAELoader",
            "inputs": {
              "vae_name": "qwen_image_vae.safetensors"
            },
            "_meta": {
              "title": "Load VAE"
            }
          },
          "4": {
            "class_type": "CLIPTextEncode",
            "inputs": {
              "text": "",
              "clip": ["2", 0]
            },
            "_meta": {
              "title": "CLIP Text Encode (Negative Prompt)"
            }
          },
          "6": {
            "class_type": "CLIPTextEncode",
            "inputs": {
              "text": "a beautiful sunset over mountains, high quality, detailed",
              "clip": ["2", 0]
            },
            "_meta": {
              "title": "CLIP Text Encode (Positive Prompt)"
            }
          },
          "7": {
            "class_type": "KSampler",
            "inputs": {
              "seed": 42,
              "steps": 4,
              "cfg": 1,
              "sampler_name": "euler",
              "scheduler": "normal",
              "denoise": 1,
              "model": ["1", 0],
              "positive": ["6", 0],
              "negative": ["4", 0],
              "latent_image": ["12", 0]
            },
            "_meta": {
              "title": "KSampler"
            }
          },
          "12": {
            "class_type": "EmptyLatentImage",
            "inputs": {
              "width": 1024,
              "height": 1024,
              "batch_size": 1
            },
            "_meta": {
              "title": "Empty Latent Image"
            }
          },
          "25": {
            "class_type": "VAEDecode",
            "inputs": {
              "samples": ["7", 0],
              "vae": ["3", 0]
            },
            "_meta": {
              "title": "VAE Decode"
            }
          },
          "26": {
            "class_type": "SaveImage",
            "inputs": {
              "filename_prefix": "qwen_output",
              "images": ["25", 0]
            },
            "_meta": {
              "title": "Save Image"
            }
          }
        }
      },
      "timeout": 120000
    },
    {
      "name": "portrait_generation",
      "input": {
        "workflow": {
          "1": {
            "class_type": "UNETLoader",
            "inputs": {
              "unet_name": "qwen_image_2512_fp8_e4m3fn_scaled_comfyui_4steps_v1.0.safetensors",
              "weight_dtype": "fp8_e4m3fn"
            },
            "_meta": {
              "title": "Load Diffusion Model"
            }
          },
          "2": {
            "class_type": "CLIPLoader",
            "inputs": {
              "clip_name": "qwen_2.5_vl_7b_fp8_scaled.safetensors",
              "type": "lumina2",
              "device": "default"
            },
            "_meta": {
              "title": "Load CLIP"
            }
          },
          "3": {
            "class_type": "VAELoader",
            "inputs": {
              "vae_name": "qwen_image_vae.safetensors"
            },
            "_meta": {
              "title": "Load VAE"
            }
          },
          "4": {
            "class_type": "CLIPTextEncode",
            "inputs": {
              "text": "blurry, low quality, distorted",
              "clip": ["2", 0]
            },
            "_meta": {
              "title": "CLIP Text Encode (Negative Prompt)"
            }
          },
          "6": {
            "class_type": "CLIPTextEncode",
            "inputs": {
              "text": "professional portrait photo of a person, studio lighting, sharp focus",
              "clip": ["2", 0]
            },
            "_meta": {
              "title": "CLIP Text Encode (Positive Prompt)"
            }
          },
          "7": {
            "class_type": "KSampler",
            "inputs": {
              "seed": 123456,
              "steps": 4,
              "cfg": 1,
              "sampler_name": "euler",
              "scheduler": "normal",
              "denoise": 1,
              "model": ["1", 0],
              "positive": ["6", 0],
              "negative": ["4", 0],
              "latent_image": ["12", 0]
            },
            "_meta": {
              "title": "KSampler"
            }
          },
          "12": {
            "class_type": "EmptyLatentImage",
            "inputs": {
              "width": 768,
              "height": 1024,
              "batch_size": 1
            },
            "_meta": {
              "title": "Empty Latent Image"
            }
          },
          "25": {
            "class_type": "VAEDecode",
            "inputs": {
              "samples": ["7", 0],
              "vae": ["3", 0]
            },
            "_meta": {
              "title": "VAE Decode"
            }
          },
          "26": {
            "class_type": "SaveImage",
            "inputs": {
              "filename_prefix": "qwen_portrait",
              "images": ["25", 0]
            },
            "_meta": {
              "title": "Save Image"
            }
          }
        }
      },
      "timeout": 120000
    }
  ],
  "config": {
    "gpuTypeId": "NVIDIA GeForce RTX 4090",
    "gpuCount": 1,
    "env": [],
    "allowedCudaVersions": [
      "12.7",
      "12.6",
      "12.5",
      "12.4",
      "12.3",
      "12.2",
      "12.1",
      "12.0"
    ]
  }
}
